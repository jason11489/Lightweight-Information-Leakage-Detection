from project import HybridDetector  # Replace with the actual module name
import ollama

loaded_hybrid = HybridDetector.load('hybrid_detector.pkl')

# ollama.pull('qwen3:latest')

# response = ollama.chat(model='qwen3:latest', messages=[
#     {'role': 'user', 'content': 'Hello, how are you?'}
# ])
# print(response['message']['content'])

print("ml model = ",loaded_hybrid.ml_model)

test_texts = [
    "ì˜¤ëŠ˜ íšŒì˜ ì˜ ë¶€íƒë“œë¦½ë‹ˆë‹¤.",
    "ê³ ê° ì£¼ë¯¼ë²ˆí˜¸ 901234-1234567 í™•ì¸ìš”ì²­",
    "íšŒì‚¬ ê¸°ë°€ ë¬¸ì„œì…ë‹ˆë‹¤. ëŒ€ì™¸ë¹„ë¡œ ì²˜ë¦¬í•´ì£¼ì„¸ìš”. ê³„ì•½ì€ ABC íšŒì‚¬ë‘ í•˜ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤.",
    "ì„œë²„ ë¹„ë°€ë²ˆí˜¸: admin123!",
    "ì ì‹¬ ì‹œê°„ì— ë­ ë¨¹ì„ê¹Œìš”?",
]

def classify_with_ollama(text):
    prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ê°€ ê°œì¸ì •ë³´ ìœ ì¶œ ìœ„í—˜ì¸ì§€ ë¶„ë¥˜í•˜ì„¸ìš”. 'ìœ ì¶œìœ„í—˜' ë˜ëŠ” 'ì •ìƒ'ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”: {text}"
    response = ollama.chat(model='qwen3:latest', messages=[
        {'role': 'user', 'content': prompt}
    ])
    return response['message']['content'].strip()

for text in test_texts:
    # HybridDetector ê²°ê³¼
    # result = loaded_hybrid.analyze(text)
    # hybrid_status = "ğŸš¨ ìœ ì¶œìœ„í—˜" if result['final']['is_leak'] else "âœ… ì •ìƒ"
    # hybrid_conf = result['final']['confidence']
    
    # Ollama ë¶„ë¥˜ ê²°ê³¼
    ollama_result = classify_with_ollama(text)
    ollama_status = "ğŸš¨ ìœ ì¶œìœ„í—˜" if "ìœ ì¶œìœ„í—˜" in ollama_result else "âœ… ì •ìƒ"
    
    print(f"Ollama: {ollama_status} | Text: {text[:40]}...")
    # print(f"Hybrid: {hybrid_status}|  Text: {text[:40]}...")