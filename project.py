"""
ê°€ë²¼ìš´ ì •ë³´ ìœ ì¶œ íƒì§€ ë¶„ë¥˜ ëª¨ë¸
- TF-IDF + ì „í†µ ML
- FastText
- ê²½ëŸ‰ ì‹ ê²½ë§
"""

import re
import pickle
from typing import List, Dict, Tuple
import warnings
warnings.filterwarnings('ignore')


# ============== 1. ê·œì¹™ ê¸°ë°˜ íƒì§€ (ê¸°ë³¸) ==============

class RuleBasedDetector:
    """ê·œì¹™ ê¸°ë°˜ ì •ë³´ ìœ ì¶œ íƒì§€ê¸°"""
    
    def __init__(self):
        self.patterns = {
            # 'ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸': r'\d{6}[-\s]?\d{7}',
            'ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸' : r'\d{2}([0]\d|[1][0-2])([0][1-9]|[1-2]\d|[3][0-1])[-]*[1-4]\d{6}',
            'ì „í™”ë²ˆí˜¸': r'(01[016789][-\s]?\d{3,4}[-\s]?\d{4})',
            'ì´ë©”ì¼': r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',
            'ì‹ ìš©ì¹´ë“œ': r'\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}',
            'ê³„ì¢Œë²ˆí˜¸': r'\d{3,4}[-\s]?\d{2,4}[-\s]?\d{4,6}',
            'IPì£¼ì†Œ': r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}',
            'ë¹„ë°€ë²ˆí˜¸íŒ¨í„´': r'(password|ë¹„ë°€ë²ˆí˜¸|pwd|passwd)[\s:=]+\S+',
        }
        
        self.sensitive_keywords = {
            'ê°œì¸ì •ë³´': ['ì£¼ë¯¼ë²ˆí˜¸', 'ì£¼ë¯¼ë“±ë¡', 'ìƒë…„ì›”ì¼', 'ì‹ ë¶„ì¦'],
            'ê¸ˆìœµì •ë³´': ['ê³„ì¢Œ', 'ì¹´ë“œë²ˆí˜¸', 'ë¹„ë°€ë²ˆí˜¸', 'ì¸ì¦ë²ˆí˜¸'],
            'ê¸°ì—…ê¸°ë°€': ['ê¸°ë°€', 'ëŒ€ì™¸ë¹„', 'ì˜ì—…ë¹„ë°€', 'ë‚´ë¶€ì •ë³´'],
            'ì ‘ê·¼ê¶Œí•œ': ['admin', 'root', 'APIí‚¤', 'secret', 'token'],
        }
    
    def analyze(self, text: str) -> Dict:
        # íŒ¨í„´ íƒì§€
        patterns_found = {}
        for name, pattern in self.patterns.items():
            if re.search(pattern, text, re.IGNORECASE):
                patterns_found[name] = True
        
        # í‚¤ì›Œë“œ íƒì§€
        keywords_found = {}
        for category, keywords in self.sensitive_keywords.items():
            matched = [kw for kw in keywords if kw.lower() in text.lower()]
            if matched:
                keywords_found[category] = matched
        
        risk_score = len(patterns_found) * 30 + len(keywords_found) * 20
        risk_score = min(risk_score, 100)
        
        return {
            'is_leak': risk_score > 30,
            'risk_score': risk_score,
            'patterns': patterns_found,
            'keywords': keywords_found,
        }


# ============== 2. TF-IDF + ì „í†µ ML (ê°€ì¥ ê°€ë²¼ì›€) ==============

class TfidfClassifier:
    """TF-IDF + Logistic Regression / Naive Bayes"""
    
    def __init__(self, model_type: str = 'logistic'):
        """
        model_type: 'logistic', 'naive_bayes'
        """
        from sklearn.feature_extraction.text import TfidfVectorizer
        from sklearn.linear_model import LogisticRegression
        from sklearn.naive_bayes import MultinomialNB
        
        self.vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 2),  # unigram + bigram
            min_df=1
        )
        
        models = {
            'logistic': LogisticRegression(max_iter=1000),
            'naive_bayes': MultinomialNB(),
        }
        self.model = models.get(model_type, LogisticRegression())
        self.model_type = model_type
        self.is_trained = False
    
    def train(self, texts: List[str], labels: List[int]):
        """í•™ìŠµ"""
        X = self.vectorizer.fit_transform(texts)
        self.model.fit(X, labels)
        self.is_trained = True
        print(f"[TF-IDF + {self.model_type}] í•™ìŠµ ì™„ë£Œ! (ìƒ˜í”Œ ìˆ˜: {len(texts)})")
    
    def predict(self, text: str) -> Dict:
        """ì˜ˆì¸¡"""
        if not self.is_trained:
            raise ValueError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        X = self.vectorizer.transform([text])
        pred = self.model.predict(X)[0]
        
        # í™•ë¥ ê°’ (ì§€ì›í•˜ëŠ” ëª¨ë¸ë§Œ)
        if hasattr(self.model, 'predict_proba'):
            proba = self.model.predict_proba(X)[0]
            confidence = proba[pred]
        else:
            confidence = 0.8 if pred == 1 else 0.2
        
        return {
            'is_leak': bool(pred),
            'label': 'ìœ ì¶œìœ„í—˜' if pred else 'ì •ìƒ',
            'confidence': float(confidence)
        }
    
    def save(self, path: str):
        with open(path, 'wb') as f:
            pickle.dump({'vectorizer': self.vectorizer, 'model': self.model}, f)
    
    def load(self, path: str):
        with open(path, 'rb') as f:
            data = pickle.load(f)
            self.vectorizer = data['vectorizer']
            self.model = data['model']
            self.is_trained = True


# ============== 3. í•˜ì´ë¸Œë¦¬ë“œ íƒì§€ê¸° ==============

class HybridDetector:
    """ê·œì¹™ ê¸°ë°˜ + ML í•˜ì´ë¸Œë¦¬ë“œ"""
    
    def __init__(self, ml_model=None):
        self.rule_detector = RuleBasedDetector()
        self.ml_model = ml_model
    
    def analyze(self, text: str) -> Dict:
        # ê·œì¹™ ê¸°ë°˜
        rule_result = self.rule_detector.analyze(text)
        
        result = {
            'text_preview': text[:80] + '...' if len(text) > 80 else text,
            'rule_based': rule_result,
        }
        
        # ML ê¸°ë°˜
        ml_result = self.ml_model.predict(text)
        result['ml_based'] = ml_result
        
        # ìµœì¢… íŒë‹¨
        result['final'] = {
            'is_leak': rule_result['is_leak'] or ml_result['is_leak'],
            'confidence': max(rule_result['risk_score']/100, ml_result['confidence'])
        }
        
        return result
    
    def save(self, path: str):
        """ëª¨ë¸ ì €ì¥"""
        import json
        
        data = {
            'rule_detector': {
                'patterns': self.rule_detector.patterns,
                'sensitive_keywords': self.rule_detector.sensitive_keywords
            },
            'ml_model': None
        }
        
        # ML ëª¨ë¸ì´ ìˆìœ¼ë©´ ì €ì¥
        if self.ml_model and hasattr(self.ml_model, 'is_trained') and self.ml_model.is_trained:
            ml_path = path.replace('.pkl', '_ml.pkl')
            self.ml_model.save(ml_path)
            data['ml_model_path'] = ml_path
            data['ml_model_type'] = getattr(self.ml_model, 'model_type', 'logistic')
        
        with open(path, 'wb') as f:
            pickle.dump(data, f)
        
        print(f"âœ… HybridDetector ì €ì¥ ì™„ë£Œ: {path}")
        
        # Chrome í™•ì¥ í”„ë¡œê·¸ë¨ìš© JSON ë‚´ë³´ë‚´ê¸°
        self._export_for_chrome(path.replace('.pkl', '_config.json'))
    
    @classmethod
    def load(cls, path: str) -> 'HybridDetector':
        """ëª¨ë¸ ë¡œë“œ"""
        with open(path, 'rb') as f:
            data = pickle.load(f)
        
        # ML ëª¨ë¸ ë¡œë“œ
        ml_model = None
        if data.get('ml_model_path'):
            ml_model = TfidfClassifier(model_type=data.get('ml_model_type', 'logistic'))
            ml_model.load(data['ml_model_path'])
        
        # HybridDetector ìƒì„±
        detector = cls(ml_model=ml_model)
        
        # ê·œì¹™ ì„¤ì • ë³µì›
        if data.get('rule_detector'):
            detector.rule_detector.patterns = data['rule_detector']['patterns']
            detector.rule_detector.sensitive_keywords = data['rule_detector']['sensitive_keywords']
        
        print(f"âœ… HybridDetector ë¡œë“œ ì™„ë£Œ: {path}")
        return detector
    
    def _export_for_chrome(self, json_path: str):
        """Chrome í™•ì¥ í”„ë¡œê·¸ë¨ìš© ì„¤ì • JSON ë‚´ë³´ë‚´ê¸°"""
        import json
        
        config = {
            'patterns': self.rule_detector.patterns,
            'sensitiveKeywords': self.rule_detector.sensitive_keywords,
            'version': '1.0',
            'exportedAt': str(import_datetime())
        }
        
        # ML ëª¨ë¸ì˜ ì¤‘ìš” íŠ¹ì„± ì¶”ì¶œ (ê°€ëŠ¥í•œ ê²½ìš°)
        if self.ml_model and hasattr(self.ml_model, 'vectorizer'):
            try:
                feature_names = self.ml_model.vectorizer.get_feature_names_out()
                
                # Logistic Regressionì˜ ê²½ìš° ì¤‘ìš” íŠ¹ì„± ì¶”ì¶œ
                if hasattr(self.ml_model.model, 'coef_'):
                    coef = self.ml_model.model.coef_[0]
                    # ìƒìœ„ 50ê°œ ìœ ì¶œ ê´€ë ¨ í‚¤ì›Œë“œ
                    top_indices = coef.argsort()[-50:][::-1]
                    top_features = [(feature_names[i], float(coef[i])) for i in top_indices]
                    config['mlFeatures'] = {
                        'topLeakKeywords': top_features[:30],
                        'modelType': self.ml_model.model_type
                    }
            except Exception as e:
                print(f"âš ï¸ ML íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Chrome í™•ì¥ í”„ë¡œê·¸ë¨ìš© ì„¤ì • ì €ì¥: {json_path}")


def import_datetime():
    from datetime import datetime
    return datetime.now().isoformat()


# ============== ìƒ˜í”Œ ë°ì´í„° ==============

SAMPLE_DATA = [
    # ========== ì •ìƒ (0) ==========
    # ì¼ìƒ ì—…ë¬´
    ("ì˜¤ëŠ˜ íšŒì˜ëŠ” 3ì‹œì— ì§„í–‰ë©ë‹ˆë‹¤.", 0),
    ("í”„ë¡œì íŠ¸ ì¼ì •ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.", 0),
    ("ì ì‹¬ ë©”ë‰´ ì¶”ì²œ ë¶€íƒë“œë¦½ë‹ˆë‹¤.", 0),
    ("ë‚´ì¼ ì¶œì¥ ì¼ì • ê³µìœ í•©ë‹ˆë‹¤.", 0),
    ("ì½”ë“œ ë¦¬ë·° ë¶€íƒë“œë¦½ë‹ˆë‹¤.", 0),
    ("ì´ë²ˆ ì£¼ ê¸ˆìš”ì¼ê¹Œì§€ ë³´ê³ ì„œ ì œì¶œ ë°”ëë‹ˆë‹¤.", 0),
    ("íšŒì˜ì‹¤ ì˜ˆì•½ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.", 0),
    ("ìƒˆë¡œìš´ ê¸°ëŠ¥ ê°œë°œì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.", 0),
    
    # ì¼ë°˜ ì†Œí†µ
    ("ì•ˆë…•í•˜ì„¸ìš”, ì˜ ì§€ë‚´ì‹œì£ ?", 0),
    ("ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”.", 0),
    ("ì£¼ë§ì— ë­ í•˜ì‹¤ ê³„íšì´ì„¸ìš”?", 0),
    ("ì»¤í”¼ í•œì” í•˜ì‹¤ë˜ìš”?", 0),
    ("ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤. ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”.", 0),
    ("ê°ì‚¬í•©ë‹ˆë‹¤. í™•ì¸í–ˆìŠµë‹ˆë‹¤.", 0),
    ("ë„¤, ì•Œê² ìŠµë‹ˆë‹¤. ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤.", 0),
    ("ì§ˆë¬¸ ìˆìœ¼ì‹œë©´ í¸í•˜ê²Œ ì—°ë½ì£¼ì„¸ìš”.", 0),
    
    # ì—…ë¬´ ê´€ë ¨
    ("ì´ë²ˆ ë¶„ê¸° ë§¤ì¶œ ëª©í‘œ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.", 0),
    ("ì‹ ê·œ ê³ ê° ìœ ì¹˜ ì „ëµ íšŒì˜ ì•ˆë‚´ë“œë¦½ë‹ˆë‹¤.", 0),
    ("ì„œë¹„ìŠ¤ ì—…ë°ì´íŠ¸ ê³µì§€ì‚¬í•­ì…ë‹ˆë‹¤.", 0),
    ("ë²„ê·¸ ìˆ˜ì • ì™„ë£Œë˜ì–´ ë°°í¬ ì˜ˆì •ì…ë‹ˆë‹¤.", 0),
    ("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì´ìƒ ì—†ìŠµë‹ˆë‹¤.", 0),
    ("ë””ìì¸ ì‹œì•ˆ ê²€í†  ë¶€íƒë“œë¦½ë‹ˆë‹¤.", 0),
    ("ë§ˆì¼€íŒ… ìº í˜ì¸ ê²°ê³¼ ë³´ê³ ì„œì…ë‹ˆë‹¤.", 0),
    ("ë‹¤ìŒ ì£¼ ì›Œí¬ìƒµ ì¥ì†Œê°€ í™•ì •ë˜ì—ˆìŠµë‹ˆë‹¤.", 0),
    
    # ê¸°ìˆ  ê´€ë ¨ (ì •ìƒ)
    ("Python 3.11 ë²„ì „ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ í–ˆìŠµë‹ˆë‹¤.", 0),
    ("ìƒˆë¡œìš´ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë„ì…ì„ ê²€í†  ì¤‘ì…ë‹ˆë‹¤.", 0),
    ("ì„±ëŠ¥ ìµœì í™” ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.", 0),
    ("ì½”ë“œ ì»¨ë²¤ì…˜ ê°€ì´ë“œ ê³µìœ ë“œë¦½ë‹ˆë‹¤.", 0),
    ("ê¹ƒí—ˆë¸Œ PR ë¦¬ë·° ìš”ì²­ë“œë¦½ë‹ˆë‹¤.", 0),
    ("CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.", 0),
    ("API ë¬¸ì„œ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤.", 0),
    ("ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 80% ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.", 0),
    
    # ========== ìœ ì¶œ ìœ„í—˜ (1) ==========
    # ê°œì¸ì •ë³´ - ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸
    ("ê³ ê° ì£¼ë¯¼ë²ˆí˜¸ëŠ” 901234-1234567ì…ë‹ˆë‹¤.", 1),
    ("ë³¸ì¸í™•ì¸ìš© ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸: 850101-2345678", 1),
    ("ì‹ ì²­ì ì£¼ë¯¼ë²ˆí˜¸ 920315-1111111 í™•ì¸ë°”ëë‹ˆë‹¤.", 1),
    ("íšŒì›ê°€ì…ì‹œ ì£¼ë¯¼ë²ˆí˜¸ ë’·ìë¦¬ í•„ìš”í•©ë‹ˆë‹¤ 880520-1234567", 1),
    
    # ê°œì¸ì •ë³´ - ì—°ë½ì²˜
    ("ê³ ê° ì—°ë½ì²˜ 010-1234-5678 ì´ë©”ì¼ test@test.com", 1),
    ("ë‹´ë‹¹ì ì „í™”ë²ˆí˜¸: 010-9876-5432ë¡œ ì—°ë½ì£¼ì„¸ìš”.", 1),
    ("ë¹„ìƒì—°ë½ë§ ê¹€OO 010-1111-2222 ë°•OO 010-3333-4444", 1),
    ("ê³ ê° ì´ë©”ì¼ ì£¼ì†Œ customer@company.com ì…ë‹ˆë‹¤.", 1),
    
    # ê¸ˆìœµì •ë³´ - ì¹´ë“œ/ê³„ì¢Œ
    ("ê³ ê° ì¹´ë“œë²ˆí˜¸ 1234-5678-9012-3456 í™•ì¸ìš”ì²­", 1),
    ("ê²°ì œ ì¹´ë“œì •ë³´: 4111-1111-1111-1111 ìœ íš¨ê¸°ê°„ 12/25", 1),
    ("í™˜ë¶ˆ ê³„ì¢Œë²ˆí˜¸ 110-123-456789 êµ­ë¯¼ì€í–‰ì…ë‹ˆë‹¤.", 1),
    ("ê¸‰ì—¬ì´ì²´ ê³„ì¢Œ ì‹ í•œ 110-456-789012ë¡œ ë³€ê²½í•´ì£¼ì„¸ìš”.", 1),
    
    # ì¸ì¦ì •ë³´ - ë¹„ë°€ë²ˆí˜¸
    ("ë°ì´í„°ë² ì´ìŠ¤ ë¹„ë°€ë²ˆí˜¸: admin123!", 1),
    ("ì„œë²„ ì ‘ì†ì •ë³´ IP 192.168.1.100 root password123", 1),
    ("FTP ì ‘ì† ë¹„ë°€ë²ˆí˜¸ëŠ” qwerty2024! ì…ë‹ˆë‹¤.", 1),
    ("ê´€ë¦¬ì ê³„ì • password: P@ssw0rd!234", 1),
    ("ì‹œìŠ¤í…œ ì´ˆê¸° ë¹„ë°€ë²ˆí˜¸ ì„¤ì •: temp1234!", 1),
    
    # ì¸ì¦ì •ë³´ - API í‚¤/í† í°
    ("API í‚¤: sk-1234567890abcdef", 1),
    ("AWS secret key: AKIAIOSFODNN7EXAMPLE", 1),
    ("GitHub í† í°: ghp_xxxxxxxxxxxxxxxxxxxx", 1),
    ("Slack webhook URL: https://hooks.slack.com/services/T00/B00/xxxx", 1),
    ("Firebase API key: AIzaSyxxxxxxxxxxxxxxxxxx", 1),
    
    # ê¸°ì—… ê¸°ë°€
    ("íšŒì‚¬ ê¸°ë°€ ë¬¸ì„œì…ë‹ˆë‹¤. ëŒ€ì™¸ë¹„ë¡œ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.", 1),
    ("ë‚´ë¶€ì •ë³´ ìœ ì¶œ ê¸ˆì§€, ì˜ì—…ë¹„ë°€ í¬í•¨ë¨", 1),
    ("ì´ ë¬¸ì„œëŠ” 1ê¸‰ ê¸°ë°€ì…ë‹ˆë‹¤. ì™¸ë¶€ ë°˜ì¶œ ê¸ˆì§€.", 1),
    ("ê²½ìŸì‚¬ ë¶„ì„ ìë£Œ - ëŒ€ì™¸ë¹„", 1),
    ("ì¸ìˆ˜í•©ë³‘ ê´€ë ¨ ê·¹ë¹„ ë¬¸ì„œì…ë‹ˆë‹¤.", 1),
    ("ì‹ ì œí’ˆ ì¶œì‹œ ê³„íš - ì‚¬ë‚´ í•œì • ê³µìœ ", 1),
    
    # ì„œë²„/ì¸í”„ë¼ ì •ë³´
    ("ìš´ì˜ì„œë²„ SSH ì ‘ì†: ssh admin@10.0.0.1 -p 22", 1),
    ("DB ì ‘ì†ì •ë³´ host: db.internal.com user: root pwd: dbpass123", 1),
    ("Redis ì„œë²„ 192.168.0.50:6379 ì¸ì¦í‚¤ redis_secret_key", 1),
    ("í”„ë¡œë•ì…˜ ì„œë²„ IP ëª©ë¡: 10.0.1.1, 10.0.1.2, 10.0.1.3", 1),
    
    # ë³µí•© ìœ ì¶œ
    ("ê³ ê°ì •ë³´ - í™ê¸¸ë™ ì£¼ë¯¼ë²ˆí˜¸ 900101-1234567 ì „í™” 010-1234-5678", 1),
    ("ê²°ì œë‚´ì—­ ì¹´ë“œ 1234-5678-9012-3456 ê¸ˆì•¡ 50000ì› ìŠ¹ì¸", 1),
    ("ê¸´ê¸‰) admin ê³„ì • ë¹„ë°€ë²ˆí˜¸ ì´ˆê¸°í™”: admin / newpass123!", 1),
    ("AWS ê³„ì • access_key: AKIA1234 secret: abcd1234efgh", 1),
]

# ============== ë©”ì¸ ì‹¤í–‰ ==============

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ”’ ê°€ë²¼ìš´ ì •ë³´ ìœ ì¶œ íƒì§€ ëª¨ë¸")
    print("=" * 60)
    
    
    # ë°ì´í„° ì¤€ë¹„
    texts = [d[0] for d in SAMPLE_DATA]
    labels = [d[1] for d in SAMPLE_DATA]
    
    # ============== ì •ëŸ‰ì  í‰ê°€ ì¶”ê°€ ==============
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.metrics import (
        confusion_matrix, 
        classification_report, 
        f1_score, 
        accuracy_score,
        precision_score,
        recall_score
    )
    
    # Train/Test ë¶„ë¦¬
    X_train, X_test, y_train, y_test = train_test_split(
        texts, labels, test_size=0.2, random_state=42, stratify=labels
    )
    
    print("\n" + "=" * 60)
    print("ğŸ“Š ì •ëŸ‰ì  í‰ê°€ (Train/Test Split)")
    print("=" * 60)
    print(f"í•™ìŠµ ë°ì´í„°: {len(X_train)}ê°œ, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test)}ê°œ\n")
    
    # ëª¨ë¸ë³„ í‰ê°€
    model_types = ['rule_based', 'logistic', 'naive_bayes']
    results = {}
    
    for model_type in model_types:
        print(f"\n{'='*50}")
        print(f"ğŸ“Œ {model_type.upper()}")
        print('='*50)
        
        if model_type == 'rule_based':
            # ê·œì¹™ ê¸°ë°˜ íƒì§€ê¸° í‰ê°€
            detector = RuleBasedDetector()
            y_pred = [int(detector.analyze(text)['is_leak']) for text in X_test]
        else:
            # TF-IDF ê¸°ë°˜ ëª¨ë¸ í‰ê°€
            clf = TfidfClassifier(model_type=model_type)
            clf.vectorizer.fit(X_train)
            print(f"\nğŸ” Vectorizer êµ¬ì„±: {clf.vectorizer.get_params()}")

            X_train_vec = clf.vectorizer.transform(X_train)
            X_test_vec = clf.vectorizer.transform(X_test)
            clf.model.fit(X_train_vec, y_train)
            clf.is_trained = True
            
            # ì˜ˆì¸¡
            y_pred = clf.model.predict(X_test_vec)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        acc = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        
        results[model_type] = {
            'accuracy': acc,
            'f1': f1,
            'precision': precision,
            'recall': recall
        }
        
        # Confusion Matrix
        cm = confusion_matrix(y_test, y_pred)
        print(f"\nğŸ”¹ Confusion Matrix:")
        print(f"              ì˜ˆì¸¡:ì •ìƒ  ì˜ˆì¸¡:ìœ ì¶œ")
        print(f"  ì‹¤ì œ:ì •ìƒ      {cm[0][0]:4d}      {cm[0][1]:4d}")
        print(f"  ì‹¤ì œ:ìœ ì¶œ      {cm[1][0]:4d}      {cm[1][1]:4d}")
        
        # Classification Report
        print(f"\nğŸ”¹ Classification Report:")
        print(classification_report(y_test, y_pred, target_names=['ì •ìƒ', 'ìœ ì¶œìœ„í—˜']))
    
    # ============== ëª¨ë¸ ë¹„êµ ìš”ì•½ ==============
    print("\n" + "=" * 60)
    print("ğŸ“ˆ ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ")
    print("=" * 60)
    print(f"{'ëª¨ë¸':<20} {'Accuracy':<12} {'F1':<12} {'Precision':<12} {'Recall':<12}")
    print("-" * 68)
    for model_type, metrics in results.items():
        print(f"{model_type:<20} {metrics['accuracy']:<12.4f} {metrics['f1']:<12.4f} {metrics['precision']:<12.4f} {metrics['recall']:<12.4f}")

    # ============== ìµœì  ëª¨ë¸ë¡œ HybridDetector ìƒì„± ë° ì €ì¥ ==============
    print("\n" + "=" * 60)
    print("ğŸ’¾ HybridDetector ëª¨ë¸ ì €ì¥")
    print("=" * 60)
    
    # ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì˜ ëª¨ë¸ ì„ íƒ (ì˜ˆ: logistic)
    best_model_type = max(results, key=lambda x: results[x]['f1'])
    print(f"ìµœì  ëª¨ë¸: {best_model_type} (F1: {results[best_model_type]['f1']:.4f})")
    
    # ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ (ê·œì¹™ ê¸°ë°˜ì€ í•™ìŠµ ë¶ˆí•„ìš”)
    if best_model_type == 'rule_based':
        best_clf = None
    else:
        best_clf = TfidfClassifier(model_type=best_model_type)
        best_clf.train(texts, labels)
    
    # HybridDetector ìƒì„± ë° ì €ì¥
    hybrid = HybridDetector(ml_model=best_clf)
    hybrid.save('hybrid_detector.pkl')
    
    # ============== HybridDetector í‰ê°€ ==============
    print("\n" + "=" * 60)
    print("ğŸ“Š HybridDetector í‰ê°€")
    print("=" * 60)
    
    # X_testì— ëŒ€í•œ ì˜ˆì¸¡
    y_pred_hybrid = [int(hybrid.analyze(text)['final']['is_leak']) for text in X_test]
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    acc_hybrid = accuracy_score(y_test, y_pred_hybrid)
    f1_hybrid = f1_score(y_test, y_pred_hybrid)
    precision_hybrid = precision_score(y_test, y_pred_hybrid)
    recall_hybrid = recall_score(y_test, y_pred_hybrid)
    
    # Confusion Matrix
    cm_hybrid = confusion_matrix(y_test, y_pred_hybrid)
    print(f"\nğŸ”¹ Confusion Matrix:")
    print(f"              ì˜ˆì¸¡:ì •ìƒ  ì˜ˆì¸¡:ìœ ì¶œ")
    print(f"  ì‹¤ì œ:ì •ìƒ      {cm_hybrid[0][0]:4d}      {cm_hybrid[0][1]:4d}")
    print(f"  ì‹¤ì œ:ìœ ì¶œ      {cm_hybrid[1][0]:4d}      {cm_hybrid[1][1]:4d}")
    
    # Classification Report
    print(f"\nğŸ”¹ Classification Report:")
    print(classification_report(y_test, y_pred_hybrid, target_names=['ì •ìƒ', 'ìœ ì¶œìœ„í—˜']))
    
    # ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
    print("\n" + "=" * 60)
    print("ğŸ”„ ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    loaded_hybrid = HybridDetector.load('hybrid_detector.pkl')
    
    print(loaded_hybrid)
    
    # í…ŒìŠ¤íŠ¸
    test_texts = [
        "ì˜¤ëŠ˜ íšŒì˜ ì˜ ë¶€íƒë“œë¦½ë‹ˆë‹¤.",
        "ê³ ê° ì£¼ë¯¼ë²ˆí˜¸ 901234-1234567 í™•ì¸ìš”ì²­",
        "íšŒì‚¬ ê¸°ë°€ ë¬¸ì„œì…ë‹ˆë‹¤. ëŒ€ì™¸ë¹„ë¡œ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.",
        "ì„œë²„ ë¹„ë°€ë²ˆí˜¸: admin123!",
    ]
    
    for text in test_texts:
        result = loaded_hybrid.analyze(text)
        print("\nresult = ",result)
        status = "ğŸš¨ ìœ ì¶œìœ„í—˜" if result['final']['is_leak'] else "âœ… ì •ìƒ"
        print(f"{status} ({result['final']['confidence']:.1%}): {text[:40]}...")